{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d9e45b-bba6-4393-b1c7-71e22cc36c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d8de5b6-f03e-477b-9068-c62aa14dadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data'\n",
    "loaders = []\n",
    "\n",
    "def pdf_loader(data_folder=DATA_FOLDER):\n",
    "    print([fn for fn in os.listdir(DATA_FOLDER) if fn.endswith('.pdf')])\n",
    "    loaders = [PyPDFLoader(os.path.join(DATA_FOLDER, fn))\n",
    "               for fn in os.listdir(DATA_FOLDER) if fn.endswith('.pdf')]\n",
    "    print(f'{len(loaders)} file loaded')\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecb6a89d-1a88-4f89-a165-49c07c7e7f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data structures and algorithms in Python.pdf', 'CompetitiveProgramming.pdf', 'Natural Language Processing with Python.pdf', 'A First Book of C++, Fourth Edition.pdf', 'Deep-Learning-with-PyTorch.pdf']\n",
      "5 file loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<langchain_community.document_loaders.pdf.PyPDFLoader at 0x13684fd50>,\n",
       " <langchain_community.document_loaders.pdf.PyPDFLoader at 0x1300f96d0>,\n",
       " <langchain_community.document_loaders.pdf.PyPDFLoader at 0x1300fa210>,\n",
       " <langchain_community.document_loaders.pdf.PyPDFLoader at 0x1300f8b50>,\n",
       " <langchain_community.document_loaders.pdf.PyPDFLoader at 0x1300f8bd0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "976e9ba4-c2c4-4789-9392-5256ec30e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, VertexAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48097964-172a-49df-90a1-82be0ced1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR API KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0945d50-e269-41f6-be47-7d92e82b2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're just building the two qa models and passing the chunk size and chunk overlap parameters to them\n",
    "# Notice that we're using their default embedding models, which is generally a good practice since each model \n",
    "# has a context length limit\n",
    "\n",
    "# The part after the arrow is the return type, could be omitted but it's a good practice, and it certainly helps in debugging\n",
    "def build_qa_chain(chunk_size: int = 1000, chunk_overlap: int = 50) -> RetrievalQA:\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    llm = OpenAI(model_name=\"text-davinci-003\",\n",
    "                temperature=0.9,\n",
    "                max_tokens=256)\n",
    "        \n",
    "    # Building the vectorstore where we will store our vector embedding of our data\n",
    "    # It is implemented using chromadb by default, which only supports similarity search only and not exact.\n",
    "    index = VectorstoreIndexCreator(embedding=embedding, text_splitter=splitter).from_loaders(loaders)\n",
    "\n",
    "    # The agent takes in the userâ€™s query, embeds the query into a vector, retrieves relevant document chunks from  \n",
    "    # the vector store, sends the relevant document chunks to the LLM and eventually passes the LLM completion to the user.\n",
    "    return RetrievalQA.from_chain_type(llm=llm, \n",
    "                                   chain_type=\"stuff\", \n",
    "                                   retriever=index.vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                   search_kwargs={\"k\": 4}),\n",
    "                                   return_source_documents=True,\n",
    "                                   input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c4d18f2-664d-45db-9aa3-12d1a126b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:129: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qa_chain = build_qa_chain(chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315501b-4da8-4b1c-b84d-b6c012e85938",
   "metadata": {},
   "source": [
    "## Understanding what each parameter is\n",
    "\n",
    "- llm: defines the LLM model to use.\n",
    "- retriever: defining from which vector store to retrieve information and by which policy. It has two additional parameters:\n",
    "- search_type: how to select the chunks from the vector store. It has two types: similarity and MMR. Similarity means selecting the most similar chunks to the query. MMR also does similarity searches. The difference is that MMR will diversify the selected chunks rather than return a very closed result.\n",
    "- search_kwargs.k: which defines the number of chunks to be selected. In the code piece above, the retriever will use a similarity search to collect 4 candidates.\n",
    "chain_type: this is specifying how the RetrievalQA should pass the chunks into LLM.\n",
    "- stuff means inserting the candidate chunks into a single prompt to send to the LLM.\n",
    "- map_reduce means sending the chunks to LLM in separated batches and comes up with the final answer based on the answers from each batch\n",
    "- refine means separating texts into batches, feeding the first batch to LLM, and feeding the answer and the second batch to LLM. It refines the answer by going through all the batches.\n",
    "- map_rerank means separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the highest-scored answers from each batch.\n",
    "return_source_documents: whether to return the document in the result. Including the documents will be helpful for understanding how the system works.\n",
    "input_key: the input is a JSON string. The input_key specifies what JSON key is leading the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00e1d3-371d-419e-aa7d-b6b2f19690a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({'question': 'What is pytorch?', 'include_run_info': True})\n",
    "print('Q:', result['question'])\n",
    "print('A:', result['result'])\n",
    "print('Resources:', result['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
